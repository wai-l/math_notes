---
title: Notebook for Statistics
jupyter: python3
format: 
    html: 
        code-fold: False
        toc: True
        toc-location: left
---


# About
This notebook is for me to record my notes in learning statistics. It includes specific math formulas, examples and how to calculate them in Python. Some notes are directly quoted from various online sources. 

# Packages used

```{python}
import math
import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import norm
from scipy.stats import t
```

# Conditional Probability

## Bayes' theorem

Bayes's theorem provides a way to update the probability of an event based on new information about the event. 

In Bayesian statistics, prior probability refers to the probability of an event before new data is collected. Posterior probability is the updated probability of an event based on new data. Posterior means occurring after. 

$$P(A \mid B) = \frac{ P(B \mid A) * P(A) }{ P(B) }$$

Event A = Rain
<br>
Event B = Cloudy

Now we are trying to get the probaility of A given event B. 
<br>
This means the probability of rain, given it's cloudy. 

This is also called posterior probability. 

$$P(Rain \mid Cloud) = \frac{ P(Cloud \mid Rain) * P(Rain) }{ P(Cloudy) }$$

Let's say below are the probability of each event: 

- Overall chance of rain = 10%
- All days start off cloudy = 40%
- Rainy days start of cloudy (P(Cloud|Rain)) = 50%

$$P(Rain \mid Cloud) = \frac{ 50\% * 10\% }{ 40\% }$$

```{python}
p_a_b = 0.5*0.1/0.4

print(p_a_b)
```

$$P(Rain \mid Cloud) = 12.5%$$

## Bayes' theorem - expaned version

Sometimes you don't know the probability of event B, which is in the denominator of the equation for the basic Bayes' theorem. In that case, you can use the expanded version of Bayes' theorem, because you don't need to know the probability of event B to use the expanded version. 

$$P(A \mid B) = \frac{P(B \mid A) * P(A)} {P(B \mid A) * P(A) + P(B \mid not A) * P(not A)}$$

Let's say we are conducting test for a medical condition. Below are the related probability: 

|Condition                   |Probability|Variable |
|----------------------------|-----------|---------|
|Having the medical condition|1%         |P(A)     |
|Test positive|unknown|P(B)|
|Given test is positive, has the medical condition|95%|P(B \| A)|
|Given test is positive, doesn't have the medical condition|2%|P(B \| not A)|
|Does not have the medical condition|99%|1-P(A)|

: Example: medical condition and test result

<br>

Now we are trying to find the probability of the person actually having the medical condition, given the test is positive. 

This would be P(A \| B). 

We cannot use the basic Bayes' theorem as the P(B) is unknown. This is when we need the expaned version. 

Putting all available variables into the formula will look like this: 
$$
P(A \mid B) = \frac{0.95 \times 0.01}{(0.95 \times 0.01) + (0.02 \times 0.99)}
$$


```{python}
P_A_B = 0.95 * 0.01 / (0.95*0.01 + 0.02 * 0.99)
print(P_A_B)
```

$$P(A \mid B) = 32.4%$$

## Case study - calculate churn rate for a customer type with Bayes' theorem (or something simplier)

Suppose you want to calculate the churn rate of a specific type of customer, e.g. UK customer. 

Given you know the number of customer that are in UK, and the number of churned customer in UK, you can easily calculate their churn rate. 

Below is a simple data set: 

|Type|Number of customer|
|------------------------------|-----------|
|Total number of customer      |100        |
|Number of churned customer    |60         |
|Number of UK customer         |50         |
|Number of churned UK customer |30         |

:Sample data: total number of customer and churned customer

$$\text{Churn rate of UK customer} = \frac{\text{Number of churned UK customer}} {\text{Number of UK customer}}$$

$$\text{Churn rate of UK customer}  = \frac{30} {50}$$

```{python}
churn_rate = 30/50
print(f'Churn rate of UK customer: {churn_rate}')
```

Now look at the Bayes' theorem
$$P(A \mid B) = \frac{ P(B \mid A) * P(A) }{ P(B) }$$

Put the numbers into the equation
$$ \text{Event A} = \text{Churn rate by all customer}$$
$$ \text{Event B} = \text{Percentage of UK customer}$$

$$
P(A \mid B) = \frac{\frac{30}{60} \times \frac{60}{100}}{\frac{50}{100}}
$$

$$P(A \mid B) = \frac{30}{60} * \frac{60}{100} * \frac{100}{50}$$
$$P(A \mid B) = \frac{30}{50}$$


```{python}
print(f'P(B|A) = {30/50}')
```

They both return the same result. 

When we know enough variables, it is sufficient for us to directly calculate the churn rate. However when we don't have enough data, or when the scenario is new, we may need to apply the Bayes' theorem to get the value we need. 

# Discrete Probability Distribution
## Binomial distribtuion

Binomial distribution referes to the calculation of the probability of k out of n ways. 

Let's say we know in a coin toss, there is a 50% chance for it to be a head or a tail. If we toss it for 10 times, what is the probability that we get 5 heads? We can use binomial distribution to calculate the value. 

Note that to use binomial distribution, the two results must be mutually exclusive. Like in a coin toss, there can only be a head or toss. 

Or in a roll of die, you either get a 3 or not get a 3. 

This is how we calculate the binomial distribution: 

$$P(x=k) = \frac{n!} {k!(n-k)!} * p^k (1-p)^{n-k}$$

The meaning of each variables: 
$$k = \text{number of success}$$

$$n = \text{number of trials}$$

$$p = \text{probability of success on a given trial}$$

We can break this down into 2 parts. 

### n-choose-k

$$\frac{n!} {k!(n-k)!}$$

This is known as n-choose-k, which refers to the number of outcome we want, or the number of success we can get at the given number of trials. 

Say you want to know out of 3 coin toss, the number of outcomes where you can get 2 heads. 

The formula would be: 

$$\text{n-choose-k}=\frac{3!} {2!(3-2)!}$$
$$\text{n-choose-k}=\frac{3*2*1} {2*1*1}$$



```{python}
n_choose_k = 3*2*1/2*1*1

print(f'n-choose-k = {n_choose_k}')
```

### Probability of each outcome
Now that we know what is the number of outcomes we can get 2 heads, we need to know the probability of each outcome as well, which will allow us to calculate the probability of x=k. 

$$p^k(1-p)^{n-k}$$

Now we know the chances of getting a head in each toss is 50%

Put it into work, it would be: 
$$0.5^2(1-0.5)^{3-2}$$

$$=0.5*0.5*0.5$$

```{python}
prob_of_each_outcome = 0.5* 0.5 * (1-0.5)

print(f'Probability of each outcome = {prob_of_each_outcome}')
```

In cases like a coin toss, where you know the chance is 50%, there is also an easier way to get the probability of each outcome. 

Say for each coin toss, you can get 2 possible outcomes. For 2 coin toss, you get 2^2, which is 4 outcome. 

Now for 3 coin toss, you will get the total possible outcome of 2^3, which is 8. 

The probability of getting each outcome is 1/8, which is also 0.125. 

```{python}
print(f'total number of possible outcome = {2**3}')
print(f'proabbility of each outcome = {1/2**3}')
```

### Final outcome
Now that we can put the two together. 

$$p(x=k) = n-choose-k * \text{probability of each outcome}$$
$$p(x=k) = 3*0.125$$


```{python}
binomial_dist = 3*0.125
print(f'p(x=k) = {binomial_dist}')
```

### (WIP)Case study: chances of customer make a return to a store

You are working for a retail store. You know that 10% of all customers visiting the store would make a return. 

Now 3 customers visit the store, what is the probability that they will return? 

Let's apply the Binomial distribution formula to all possible situations. 

Result should be like below: 

|Number of success|P     |
|-----------------|------|
|p(x=0)           |0.729 |
|p(x=1)           |0.243 |
|p(x=2)           |0.027 |

:Value of p(x=k) for different number of success

## Poisson distribution
Possion distribution models the probability that a certain number of events will occur during a specific time period. 

Below are some practical use cases

- Calls per hour for a customer service call centre
- Visitors per hour for a website
- Customers per day at a restaurant

Below are necessary for a Poisson experiment

- The number of events in the experiment can be counted
- The mean number of events that occur during a specific time period is known
- Each event is independent

### Poisson distribution formula
$$p(x=k) = \frac{λ^k e^{-λ}}{k!}$$

- λ (lambda) is the mean number of events that occur during a specific time period
- k refers to the number of events
- e: a constant equal to approximately 2.71828
- e^-λ: 1/e^λ

### Example: possible number of order in a drive through restaurant (WIP)
Let's say we know that the average number of order per minute is 2. 

We can use the Poisson formula to determine the probability of the restaurant receiving 0, 1, 2 or 3 orders per minute. 

|Number of orders |P     |
|-----------------|-------|
|p(x=0)           |0.1353 |
|p(x=1)           |0.2707 |
|p(x=2)           |0.2707 |
|p(x=3)           |0.1805 |

: probability of the restaurant receiving number of orders

## Possion VS Binomial

|   |Given|Want to Find|
|--------|-----------------------|-----------------------|
|Poisson |The average probability of an event happening for a specific time period|The probability of a certain number of events happening in that time period|
|Binomial|An exact probability of an event happening|The probability of the event happening a certain number of times in a repeated trial|

:Possion VS Binomial

# Continuous Probability
## The normal distribution
This is also know as the bell curve. It has the following features: 

- The shape is a bell curve
- The mean is located at the centre of the curve
- The curve is symmetrical on both sides of the center
- The total area under the curve equals 1

The bell curve follows the emprical rule

- 68% of values fall within 1 standard deviation of the mean
- 95% of values fall within 2 standard deviation of the mean
- 99.7% of values fall within 3 standard deviation of the mean

## Z-score
Z-score allows us to compare the normal distribution of different dataset by standardise the normal distribution. It looks at the normal distribution in terms of standard deviation. This is also known as standarised normal distribution

![Standarised normal distribution](images/z-score_1.png)

![Z-score allows us to compare the normla distribution in different datasets](images/z-score_2.png)

### Formula - mean
This is how we calculate the Z-score: 
$$Z = \frac{x-μ}{σ}$$
$$x = \text{a single data value/raw score}$$
$$μ/mu = \text{the population mean}$$
$$σ/sigma = \text{the poplation standard deviation}$$

Now let's say you have a test score of 133. The test has a mean score of 100 and a standard deviation of 15. The equation would look like this: 
$$Z = \frac{133-100}{15}$$


```{python}
z_score = (133-100)/15
print(f'Z-score: {z_score}')
```

### Formula - proportion
$$Z = \frac{\hat{P}-P}{\sqrt{\frac{\hat{P}(1-\hat{P})}{n}}}$$

$$\hat{P} = \text{Obverved sample proportion}$$
$$P = \text{Population proportion}$$
$$n = \text{Sample size}$$

This measures the standard error of the proportion. Note that if we are calculating the Z-score for a hypothesis test, you may replace the sample proportion (p hat) with the population proportion (p). In hypothesis testing, we are testing whether the true population proportion P is equal to the hypothesized value. Therefore, we calculate the standard error assuming the null hypothesis is true and use P to determine how much variability we expect in the sample proportions.
$$\sqrt{\frac{\hat{P}(1-\hat{P})}{n}} = \text{Sample error of the proportion}$$


### Python
You can use the z-score function in the scipy package to calculate the z-score of each datapoint in your dataframe. 

## Smaller sample size - T-score and T-distribution
The Student's t-distribution, often just called the t-distribution, is a probability distribution that is used when estimating the mean of a normally distributed population in situations where the sample size is small, and the population standard deviation is unknown. This distribution was developed by William Sealy Gosset under the pseudonym "Student." 

It is similar to the normal distribution but more compressed with heavier tails. 

![Z-distrbution vs T-distribution](images/t-distrbution.png)

Below are the differences between T-score and Z-score: 

|Feature      |Z-Score                   |T-Score                   |
|-------------|--------------------------|--------------------------|
|Sample Size  |Large samples (n>30n>30)|Small samples (n≤30n≤30)|
|Population Standard Deviation|Known|Unknown|
|Distribution |Normal distribution|T-distribution (heavier tails)|
|Applications |Standardized tests, quality control, large-scale surveys|Small-sample studies, medical trials, social science experiments|
|Type of Data |Population data or large samples|Sample data when population parameters are unknown|

:T-score VS Z-score

# Sampling distribution
## Standard Error of the Mean
The standard error formula is base on the math assumption of repeated sampling, and it calculates the possible range of the actual mean based on one sample. 

$$\text{Standard error of the mean} = \frac{σ*}{\sqrt{n}}$$
$$σ = \text{Standard deviation of the population}$$

*The standard deviation of the population is usually unknown, so in practice we often use the standard deviation o the sample instead. 

$$\text{Standard error of the mean} = \frac{s}{\sqrt{n}}$$
$$s = \text{The sample standard deviation}$$
$$n = \text{The sample size}$$

The larger the sample size is,the smaller the error would be. 

Now let's say we have a sample of 100 penguine, which has a mean weight of 3 pounds and a standard deviation of 1 pound. 

$$\text{Stand error of the mean} = \frac{1}{\sqrt{100}}$$

```{python}
sample_std = 1
n_sqr = math.sqrt(100)

standard_error = sample_std/n_sqr

print(standard_error)
```

Now while the best estimated mean of the population will still be 3 pounds, you know that the mean will vary with a standard deviation of 0.1 pound. 

## The Central Limit Theorem
The sample distribution of the mean approaches a normal distribution when the sample size increases. 

## Standard Error of the Proportion
### Theory and formula
The standard error of proportion measures the standard error of a sampling proportion. 

$$SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$

$$\hat{p} (\text{p hat}) = \text{population proportion}$$

$$n = \text{sample size}$$

Say you are researching a population of 100,000, in a sample with 100 people, 10% of them prefer slipon sneaker. 

$$SE(\hat{p}) = \sqrt{\frac{0.1(1-0.1)}{100}}$$


```{python}
import math

phat = 0.1
sample = 100
sample_error = math.sqrt(0.1*(1-0.1)/100)

print(sample_error)

```

The sample error is 0.03, which means that the estimate population proportion who prefer slipon sneaker is 10%, which would deviate for +/-3%. 

### Case study: apply the standard error of the proportion to churn rate
Say you know that you have 100 customers (current and churned) in Scotland, and their churn rate is 60%. You can use the standard error formula to understand how much the sample proportion (in your case, the sample churn rate) is expected to vary from the true population proportion (the true churn rate for all current and future customers). 


```{python}
import math

# Given data
p_hat = 0.60  # Sample churn rate
n = 100       # Sample size

# Standard error calculation
standard_error = math.sqrt((p_hat * (1 - p_hat)) / n)

print(standard_error)
```

The standard error is around 0.05, which means that the standard deviation of the churn rate for current and future Scotland customer will be at 60%, vary for +-5%. 

You can further calculate the confidence level base on the standard error and sample size, to understand how reliable this is. 

# Confidence level
Imagine you want to estimate the mean weight of a population of 10,000 penguins. Instead of weighing every single penguin, you select a sample of 100 penguins. The mean weight of your sample is 30 pounds. Based on your sample data, you construct a 95% confidence interval between 28 pounds and 32 pounds. 

95 CI [28, 32]

Usual confidence level choosen are as follow: 

- 90%
- 95%
- 99%

## Interpret the confidence interval
Technically, 95% confidence means that if you take repeated random samples from a population, and construct a confidence interval for each sample using the same method, you can expect that 95% of these intervals will capture the population mean. You can also expect that 5% of the total will not capture the population mean. 

The confidence level refers to the long-term success rate of the method, or the estimation process based on random sampling. 

Imagine you take 20 random samples of 100 penguins each from the penguin population, and calculate a 95% confidence interval for each sample. You can expect that approximately 19 of the 20 intervals, or 95% of the total, will contain the actual population mean weight of 31 pounds. One such interval will be the range of values between 28 pounds and 32 pounds. 

![Interpret the confidence interval](images/confidence_intervals.png)

## Incorrect interpretations

- There is a 95% probability that the population mean falls within the constructed interval. (It's not true!)

What you can say is that if you take repeated random samples from the population, and construct a confidence interval for each sample using the same method, you can expect 95% of your intervals to capture the population mean. 

Pro tip: Remember that a 95% confidence level refers to the success rate of the estimation process. 

- 95% of all of the data values in the population fall within the interval (Also not true. )

## Construct confidence interval for porportion

1. Identify a sample statistic
2. Choose a confidence level
3. Find the margin of error
4. Calculate the interval

Let's say you work for election polling. Below would be the associate data point. 


|Component        |Metric           |Data|
|-----------------|-----------------|------|
|Sample statistic |Percentage of sample that prefer candidate A|55%|
|Confidence level |95%|1.96|
|Standard error   |Standard error of poportion from the sample (assume sample size is 100)|5%|
|Margin of error  |Assume a large sample size and normal distribution, it would be z-score * standard error|1.96*0.05=0.098|
|Upper limit of the interval|Sample stat + margin of error|55%+9.8%=64.8%|
|Lower limit of the interval|Sample stat - margin of error|55%-9.8%=45.2%|
|Confidence Interval|Upper limit - lower limit|64.8%-45.2%=19.6%|

: Construct confidence interval for election polling

|Confidence Level |Z-score |
|------------|------------|
|90% |1.645 |
|95% |1.96 |
|99% |2.58 |

: Common confidence level and associated z-score

The formula for calculate margin of error in a large sample size would be: 

$$\text{Margin of error} = \text{z-score}*\text{standard error}$$

Note that in a smaller sample size, we will need to use **t-score** to calculate the margin of error. 

In the situation of polling, we will use the standard error of poportion for the standard error calculation: 

$$SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$

$$\text{Sample proportion}=0.55$$
$$\text{Sample size}=100$$


```{python}
import math
p=0.55
n=100

se = math.sqrt(p*(1-p)/n)

print(f'Standard error of porportion: {se}')
```

## Construct confidence interval for mean
1. Identify a sample statistic
2. Choose a confidence level
3. Find the margin of error
4. Calculate the interval

Let's say this time you work for a a cellphone manufactory company, and you are trying to get the average battery life of a new cellphone battery. 


|Component        |Metric           |Data|
|-----------------|-----------------|------|
|Sample statistic |Mean|20.5|
|Confidence level |95%|1.96|
|Standard error   |Standard error of poportion from the sample (assume sample size is 100)|0.15|
|Margin of error  |Assume a large sample size and normal distribution, it would be z-score * standard error|1.96*0.15=0.294|
|Upper limit of the interval|Sample stat + margin of error|20.5+0.294=20.794|
|Lower limit of the interval|Sample stat - margin of error|20.5-0.294=20.206|
|Confidence Interval|Upper limit - lower limit|20.794-20.206|

: Construct confidence interval for cellphone battery

We want to use the standard error of the mean to calculate the margin of error. 

As we know the standard deviation of the population, we can use that for the formula. 
$$\text{Standard error of the mean} = \frac{σ}{\sqrt{n}}$$


```{python}
import math

sigma=1.5
n=100

se = 1.5/math.sqrt(100)

print(f'Standard error: {se}')
```

## Calculate confidence interval with SciPy
First let's generate a random sample of a survey result of government support rate by town. 


```{python}
import pandas as pd
import numpy as np

# Set seed for reproducibility
np.random.seed(42)

# Number of rows
n_rows = 500

# Generate town names (using 'Town' with an index for uniqueness)
towns = [f'Town_{i+1}' for i in range(n_rows)]

# Generate random support rates (between 0 and 100)
support_rates = np.random.uniform(0, 1, n_rows).round(2)

# Create the DataFrame
data = pd.DataFrame({
    'town': towns,
    'support_rate': support_rates
})

data.head()  # Display the first few rows of the dataset

data.describe()

data['support_rate'].mean()

```

The average support rate by town for the whole data set is 0.4987. 

Now take a random sample size of 50. 

```{python}
data_sample = data.sample(n=50, replace=True, random_state=30624300)
```

To get the confidence interval of the sample, we will need to know the below: 

- Confidence level
- Sample mean
- Sample standard error

Assume we want the confidence level of 95 or 99. 

```{python}
confi_level = 0.95

sample_mean = data_sample['support_rate'].mean()

print(f'Sample mean: {sample_mean}')
print()

standard_error = data_sample['support_rate'].std()/np.sqrt(data_sample.shape[0])

print(f'Standard error: {standard_error}')
print()

conf_interval = stats.norm.interval(confi_level, loc=sample_mean, scale=standard_error)

print(f'Confidence interval of the sample with confidence level of {confi_level}: ')
print(conf_interval)

print()
confi_level = 0.99
print(f'Confidence interval of the sample with confidence level of {confi_level}: ')
print(conf_interval)

```

# Hypotheses testing 
## Steps for conducting a hypothesis test

1. State the null hypothesis and the alternative hypothesis.

2. Choose a significant level.

3. Find the p-value.

4. Reject or fail to reject the null hypothesis. (by comparing p-value and significant level)


## Null VS Alternative hypothesis 

+--------------+--------------------------------------+--------------------------------------+
|              |**Null hypothesis (H0)**              |**Alternative hypothesis (Ha)**       |
+==============+======================================+======================================+
|**Claims**    |There is no effect in the population. |There is an effect in the population. |
+--------------+--------------------------------------+--------------------------------------+
|**Assumption**|Assumed to be true unless there is    |Accepted as true only if there is     |
|              |convincing evidence to the contrary.  |convincing evidence for it.           |
+--------------+--------------------------------------+--------------------------------------+
|**Language**  |- No Effect                           |- An effect                           |
|              |- No difference                       |- A difference                        |
|              |- No relationship                     |- A relationship                      |
|              |- No change                           |- A change                            |
+--------------+--------------------------------------+--------------------------------------+
|**Symbols**   |Equality (=, ≤, ≥)                    |Inequality (≠, <, >)                  |
+--------------+--------------------------------------+--------------------------------------+

: Null VS Alternative hypothesis

## Significant level
This is the threshold at which we will consider our result statiscally significant. It is also the probability of rejecting the null hypothesis when it is true. 

5% is the most common standard for significant level. 

For more strict result, 1% is used. 

## P-value
The probability of observing a difference in your result as or more extreme than the difference observed when the null hypothesis is true. 

It is used to compared against the significant level. 

The p-value is calculated by test statistic, a value that shows how closely your observed data matches the distribution expected under the null hypothesis. After getting a t-score or a z-score from a t-test or z-test, you can calculate the p-value by 

## Examples of Null and Alternative hypothesis
### Example#1: Mean weight 

An organic food company is famous for their granola. The company claims each bag they produce contains 300 grams of granola—no more and no less. To test this claim, a quality control expert measures the weight of a random sample of 40 bags.

- H0: μ = 300 (the mean weight of all produced granola bags is equal to 300 grams)
- Ha: μ ≠ 300 (the mean weight of all produced granola bags is not equal to 300 grams)

### Example#2: Mean height 

Suppose it’s assumed that the mean height of a certain species of tree is 30 feet tall. However, one ecologist claims the actual mean height is greater than 30 feet. To test this claim, the ecologist measures the height of a random sample of 50 trees.

- H0: μ ≤ 30 (the mean height of this species of tree is equal to or less than 30 feet)
- Ha: μ > 30 (the mean height of this species of tree is greater than 30 feet)

### Example#3: Proportion of employees

A corporation claims that at least 80% of all employees are satisfied with their job. However, an independent researcher believes that less than 80% of all employees are satisfied with their job. To test this claim, the researcher surveys a random sample of 100 employees.

- H0: p ≥ 0.80 (the proportion of all employees who are satisfied with their job is equal to or greater than 80%)
- Ha: p < 0.80 (the proportion of all employees who are satisfied with their job is less than 80%)

## Type I and Type II errors
When you decide to reject or fail to reject the null hypothesis, there are four possible outcomes–two represent correct choices, and two represent errors. You can: 

- Reject the null hypothesis when it’s actually true (Type I error)
- Reject the null hypothesis when it’s actually false (Correct)
- Fail to reject the null hypothesis when it’s actually true (Correct) 
- Fail to reject the null hypothesis when it’s actually false (Type II error)

## One sample tests - T-test vs z-test
One-sample tests are usually used to determine whether a population mean is equal to a specific value. 

The primary difference between the one-tail t-test and the one-tail z-test lies in the distribution they use, which depends on whether the population standard deviation is known and the sample size. Both tests are used to determine if there is a significant difference between the sample mean and the population mean, but they differ slightly in their formula.

### One tail t-test
The t-test is typically used when:

- The sample size is small (generally n<30).
- The population standard deviation σ is unknown, so we estimate it using the sample standard deviation s.

The formula for the t-test statistic is: 
$$t = \frac{\bar{x}-μ}{\frac{s}{\sqrt{n}}}$$

 
Where: 

- x̄ = sample mean
- μ = population mean (hypothesized)
- s = sample standard deviation
- n = sample size
- s/sqrt(n) = standard error of the sample mean

The result is compared with a t-distribution with n−1 degrees of freedom to get the critical value for a one-tail test. 

### One tail z-test
The z-test is typically used when:

- The sample size is small (generally n<30).
- The population standard deviation σ is unknown, so we estimate it using the sample standard deviation s.

- The sample size is large (typically n≥30).
- The population standard deviation σ is known.

The formula for the z-test statistic is: 
$$z = \frac{\bar{x}-μ}{\frac{σ}{\sqrt{n}}}$$

 
Where:

- x̄ = sample mean
- μ = population mean (hypothesized)
- σ = population standard deviation
- n = sample size
- σ/sqrt(n) = standard error of the population mean

The result is compared with the standard normal distribution (Z-distribution) to get the critical value for a one-tail test.

## Calculate the p-value from a one-tail/two-tail z-score and t-score
### Z-score
**Two-tail z-test**
$$\text{p-value} = 2 \times P(Z \geq |z|)$$

**Right-tail z-test**
$$\text{p-value} = P(Z \geq z)$$

**Left-tail z-test**
$$\text{p-value} = P(Z \leq z)$$

**Example**
Suppose z = 1.96 in a two-tail test. 

$$P(Z \geq 1.96) = 0.025$$
$$\text{p-value} = 2*0.025 = 0.05$$

```{python}
from scipy.stats import norm
print((1-norm.cdf(abs(1.96)))*2)
```

**Calulation in python**

```{python}
from scipy.stats import norm

# Given z-score
z = 1.96

# Two-tailed test p-value
p_value_two_tailed = 2 * (1 - norm.cdf(abs(z)))

# Right-tailed test p-value
p_value_right_tailed = 1 - norm.cdf(z)

# Left-tailed test p-value
p_value_left_tailed = norm.cdf(z)

print("Two-tailed p-value:", p_value_two_tailed)
print("Right-tailed p-value:", p_value_right_tailed)
print("Left-tailed p-value:", p_value_left_tailed)

```

### T-score
In a t-test, you also want to determine the degree of freedom (df). In a one sample t-test, it would usually be n-1. 
$$df = n-1$$

Below are some different cases of degree of freedom baseon the test we conduct: 

|Test                                  |Degrees of Freedom (df)                               |
|--------------------------------------|------------------------------------------------------|
|One-Sample t-Test                     |n−1                                                   |
|Paired t-Test                         |n−1                                                   |
|Two-Sample t-Test (Equal Variances)   |n1+n2−2                                               |
|Two-Sample t-Test (Unequal Variances) |Approximate df based on Welch-Satterthwaite equation  |
|ANOVA (Between Groups)                |k−1                                                   |
|ANOVA (Within Groups)                 |N−k                                                   |
|Chi-Square (Goodness-of-Fit)          |k−1                                                   |
|Chi-Square (Independence)             |(r−1)(c−1)                                            |
|Linear Regression (Residual)          |n−p−1                                                 |
|F-Test                                |df1,df2       ​                                        |

: Degree of freedom in different stat tests

**Two-tail t-test**
$$\text{p-value} = 2 \times P(T \geq |t|;df)$$

**Right-tail t-test**
$$\text{p-value} = P(T \geq t;df)$$

**Left-tail t-test**
$$\text{p-value} = P(T \leq t;df)$$

**Calculation in python**

```{python}
from scipy.stats import t

# Given t-score and degrees of freedom
t_score = 2.0
df = 10  # degrees of freedom

# Two-tailed test p-value
p_value_two_tailed_t = 2 * (1 - t.cdf(abs(t_score), df))

# Right-tailed test p-value
p_value_right_tailed_t = 1 - t.cdf(t_score, df)

# Left-tailed test p-value
p_value_left_tailed_t = t.cdf(t_score, df)

print("Two-tailed p-value (t-test):", p_value_two_tailed_t)
print("Right-tailed p-value (t-test):", p_value_right_tailed_t)
print("Left-tailed p-value (t-test):", p_value_left_tailed_t)

```

```{{python}}
stats.ttest_1samp(a = aqi_michigan['aqi'], popmean=10, alternative='greater')
```

## One sample tests - Z-test
### Assumption: 

- The data is a random sample of a normally distributed population
- The standard deviation of the population is known

### Case Study: Online Delivery
Let's say you work for an online delivery company. Currently the average delivery time is 40 minutes, with a standard deviation of 5 minutes. 

Recently, the company launched a new training program to make the delivery more efficient. After the drivers completed the training program, the company tracked a random sample of 50 deliveries to understand how long a delivery take. The sample has an average delivery time of 38 minutes, and a standard deviation of 5 minutes. 

Now the company wants to know if the decrease in delivery time is due to chance, or is statistically significant. 

We can determine this with a one hypothesis z-test. 

**Population**

- Mean = 40 min
- Standard deviation = 5 min

**Sample**

- Mean = 38 min
- Standard deviation = 5 min

Let's build the hypothesis step by step: 

**Null hypothesis**

The average delivery time equals 40 minutes. 

**Alternative hypothesis**

The average delivery time is less than 40 minutes. 

**Significant level**

5%

This is the threshold at which we will consider our result statiscally significant. It is also the probability of rejecting the null hypothesis when it is true. 

**P-value**

The probability of observing a difference in your result as or more extreme than the difference observed when the null hypothesis is true. 

The probability of observing a difference that is two minutes or greater if the null hypothesis is true. If the probability of this outcome is very unlikely, in particular, if your p-value is less than your significance level of five percent, then you'll reject the null hypothesis. 

If p-value < 5%, then reject the null hypothesis. 

In this case, as we are using z-test, we will calculate the z-score to find the p-value. 

$$z = \frac{\bar{x}-μ}{\frac{σ}{\sqrt{n}}}$$

Where: 

- x̄ = sample mean
- μ = population mean (hypothesized)
- σ = population standard deviation
- n = sample size
- σ/sqrt(n) = standard error of the population mean

$$z = \frac{38-40}{\frac{5}{\sqrt{50}}}$$


```{python}
import math
from scipy.stats import norm

sample_mean = 38
pop_mean = 40
pop_std = 5
sample_size = 50

z_score = (sample_mean-pop_mean)/(pop_std/math.sqrt(sample_size))

p_value = norm.cdf(z_score)

print(f'Z-score: {z_score}')
print(f'P-value: {p_value}')

```

In this case, the z-score is -2.8284. 
P-value is 0.0023 or 0.23%. 

In this case, it is very unlikely that the mean delivery time is 38 minutes when the null hypothesis is true. In other words, it's highly unlikely that the sample mean of 38 is due to chance. Therefore we reject the null hypothesis. 

The p-value we calculated here is the probability for when the z-score <= -2.8284 in a normal distribution, which is the red area in the image below. 

![P value according to the z-score](images/z_test_p_value.png)

## Two-sample test
A two-sample test determines whether two population means are equal to each other. It is frequently used in A/B testing. 

## Two-sample t-test for means
### Assumption

- The two samples are independent of each other
- For each sample, the data is drawn randomly from a normally distributed population
- The population standard deviation is unknown

### Hypothesis

- Null: There is no difference in the mean on version A and version B
- Alternative: There is a difference in the mean on version A and version B
- Significant level: Usually 5%, can also be 1% or 10%
- P-value: The probability of observing a difference in your sample means as or more extreme than the difference observed when the null hypothesis is true 

### Code
If you have the full sample data, you can leverage Scipy's t-test functions to calculate the t-score and p-value as below: 

```{{python}}
stats.ttest_ind(a=sampled_state21['OVERALL_LI'], b=sampled_state28['OVERALL_LI'], equal_var=False, alternative='two-sided')
```

It should return the t-score and p-value like below: 
Ttest_indResult(statistic=2.8980444277268735, pvalue=0.006421719142765231)

### T-score for two-sample t-test

$$t = \frac{(\bar{x}_1 - \bar{x}_2)}{\sqrt{({\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}})}}$$


- x̄ = sample mean
- s = sample standard deviation
- n = sample size
- s^2/n = variance of the sample mean

### Get p-value from two-sample t-test
In a two-sample t-test, you would want the below: 
```{python}
from scipy.stats import t

# Given t-score and degrees of freedom
t_score = -1.2541
df = 40+38-2  # degrees of freedom

# Two-tailed test p-value
p_value_two_tailed_t = 2 * (1 - t.cdf(abs(t_score), df))

print("Two-tailed p-value (t-test):", p_value_two_tailed_t)

```


### Case study - Landing page A/B test
You are working for a cosmetics company. The company is researching the amount of time the customers spent on its website. Your team leader ask you to conduct a A/B test to determine if changing the background color of the landing page from grey to green has any effect on the average time spent on the page. 

You randomly select two groups of users. The first group visits the gray landing page named version A. The second group visits the green landing page named version B. 

Below are the data you collected: 

|                  |Version A   |Version B   |
|------------------|------------|------------|
|Landing page color|Grey        |Green       |
|Sample size       |40          |38          |
|Average time spent|300 seconds |305 seconds |
|Standard deviation|18.5 seconds|16.7 seconds|

: Version A VS Version B

**Hypothesis**

- Null: There is no difference in the mean time spent on version A and version B
- Alternative: There is a difference in the mean time spent on version A and version B
- Significant level: 5%
- P value: The probability of observing a difference in your sample means as or more extreme (5 seconds) than the difference observed when the null hypothesis is true 

**Calculation**

$$t = \frac{(\bar{x}_1 - \bar{x}_2)}{\sqrt{({\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}})}}$$


- x̄ = sample mean
- s = sample standard deviation
- n = sample size
- s^2/n = variance of the sample mean

$$t = \frac{(300 - 305)}{\sqrt{({\frac{18.5^2}{40} + \frac{16.7^2}{38}})}}$$

```{python}
import math

x1 = 300
x2 = 305
s1 = 18.5
s2 = 16.7
n1 = 40
n2 = 38

t = (x1-x2)/math.sqrt(s1**2/n1 + s2**2/n2)

print(f'T-score: {t}')
```

T-score: -1.254103693454824

Now you want to get the probability of the red area from the image below, which is lower than -1.2541 and greater than 1.2541

![Two-tail test p-value](images/two_tails_test_p_value.png)



```{python}
from scipy.stats import t

# Given t-score and degrees of freedom
t_score = -1.2541
df = 40+38-2  # degrees of freedom

# Two-tailed test p-value
p_value_two_tailed_t = 2 * (1 - t.cdf(abs(t_score), df))

print("Two-tailed p-value (t-test):", p_value_two_tailed_t)

```

P-value = 0.2136 or 21.36%

While p-value (21.36%) > significant level (5%), we fail to reject the null hypothesis, and conclude there is no statistically significant differece between the two mean. 

## Z-test for proportion
We use z-test for proportion as t-test does not apply to proportion for technical reasons. 

Z-test for proportion can be used to compare: 

- Defects among manufacturing products on two assembly lines
- Side effects to a new medicine for two trial groups
- Support for new law among voters in two disctricts

### Two tail z-test for proportion case study - Customer satisfaction
Suppose the company claims that 80% of its customers are satisfied with their shopping experience. To test this claim, you survey a random sample of 100 customers. According to the survey, 73% of customers say they are satisfied. Based on the survey data, you conduct a z-test to evaluate the claim that 80% of customers are satisfied. 

**Hypothesis**

- Null: P = 0.80 (the proportion of satisfied customers equals 80%)
- Alternative: P ≠ 0.80 (the proportion of satisfied customers does not equal 80%)
- Significant level: 5%

**Z-score for proportion in hypothesis teseting**
$$Z = \frac{\hat{P}-P}{\sqrt{\frac{{P}(1-{P})}{n}}}$$

```{python}
from scipy.stats import norm
import math

sample_mean = 0.73
pop_mean = 0.80
sample_size = 100

z_score = (sample_mean-pop_mean)/math.sqrt(pop_mean*(1-pop_mean)/sample_size)

print(f'Z-score: {z_score}')

# Two-tailed test p-value
p_value_two_tailed = 2 * (1 - norm.cdf(abs(z_score)))

print("Two-tailed p-value:", p_value_two_tailed)

```

P-value = 0.08

As P-value > significant level, you failed to reject the null hypothesis. 

Note that the p-value of 0.08 does not mean there is an 8% probability that the sample proportion is 0.80. Instead, it means there is an 8% probability of observing a sample proportion as extreme as 0.73 or more extreme, given that the true population proportion is 0.80.

To put it in simple term, the p-value answers the question: "If the true population proportion is 0.80, what is the probability of observing a sample proportion as different from 0.80 as 0.73 is?"

### Two-sample z-test for proportion case study - Employee satisfaction
Let's say the HR team of an international company conducted an employee satisfaction survey for their London and Beijing office. Both has a sample size of 50. The satisfaction rate of the London office is 67%, Beijing is 57%. We want to find out if the 10% difference is due to chance. 

**Hypothesis**

- Null (H0): There is no difference in the proportion of the satisfied employees in London and Beijing
- Alternative (Ha): There is a difference in the proportion of the satisfied employees in London and Beijing
- Significant level: 5%
- P-value: The probability of observing a difference in your sample proportion as or more extreme than the difference observed when the null hypothesis is true

In this case, we will reject the null hypothesis when p-value < 5%. 

**Z-score for two sample proportion**
$$Z=\frac{\hat{p_1}-\hat{p_2}}{\sqrt{\hat{p_0}(1-\hat{p_0})(\frac{1}{n_1}+\frac{1}{n_2})}}$$

$$\hat{p_1} = \text{Sample proportion of the 1st group}$$
$$\hat{p_2} = \text{Sample proportion of the 2nd group}$$

$$n_1 = \text{Sample size of the 1st group}$$
$$n_2 = \text{Sample size of the 2nd group}$$
$$\hat{p_0} = \text{Pooled proportion (weighted average of the two samples)}$$
$$\text{Pooled proportion} = \frac{x_1+x_2}{n_1+n_2}$$
$$x_1 = \text{Number of success in sample 1}$$
$$x_2 = \text{Number of success in sample 2}$$


```{python}
import math
from scipy.stats import norm

p_1 = 0.67
p_2 = 0.57
p_0 = (0.67+0.57)/2
x_1 = 50
x_2 = 50

z = (p_1 - p_2)/math.sqrt(p_0*(1-p_0)*(1/x_1+1/x_2))

print(f'Z score: {z}')

# Two-tailed test p-value
p_value = 2 * (1 - norm.cdf(abs(z)))

print("Two-tailed p-value:", p_value)
```

Z score: 1.0301070542879125
Two-tailed p-value: 0.30295975411763143

The probability that thereis a 10% difference between two offices in London and Beijing is 30.3% under the null hypothesis. This is greater than the significant level, therefore we failed to reject the null hypothesis, and concluded that there is no statistical significant in the percentage difference between Beijing and London. 

# Linear regression
## Simple linear regression
Linear regression model explore how one or multiple independent variable impact the dependent variable. 


**Equation**
$$Y = \text{intercept} + \text{slope}*X$$

Or it can be written as

$$\mu_{y|x}=\beta_0 + \beta_1 * X$$

It's not always possible to get all the x and y variables in a population. For example if you want to evaluate the impact of number of social media followers against book sales, you can't get all author in the world for your linear regression model. This is when we use a sample of the population to build the regression model. (estimate of the paramaters). To distinguish the differences, the equation has a hat on the estimate parameters. 

$$
\hat{y} = \hat{\beta_0} + \hat{\beta_1} x
$$

**Independent variable (x)**
A variable that explains trends in the dependent variable, also referred to as an explanatory or predictor variable. 

**Dependent variable (y)**
The variable a given model estimates, also referred to as a response or outcome variable. 

**Slope**
The amount that y increases or decreases per one-unit increase of x. 

**Intercept**
The value of y when x equals 0. 

**Correlation**
Describes linear relationships between variables (positive/negative). 

**Linear regression coefficients**
The B-hat shown in the estimated formula are known as regression coefficients. A common way to caculate it is Ordinary Least Squares Estimation (OLS). 

### Best fitted line
In a simaple linear regression model, we want to find the best fitted line, which is the line that fits the data best by minimizing some loss function or error. 

There are a few key components associated with this concept. 

**Error**

Some differences between the data we have, the observed values and the predicted value generated by a given model. 

**Predicted Value** 
The estimated Y values for each X caculated by a model. 

**Residual**
The difference between observed or actual values and the predicted values of the regression line. We can calculate the residual for each data point. It's important to note that the sum of residuals in always equal to 0 for **OLS (Oridnary Least Squares)estimators**. This is not always true for other estimators. 

The equation for residual is: 
$$\epsilon_i=y_i - \hat{y_i}$$

$$\epsilon = \text{residual}$$

$$y_i = \text{observed}$$

$$\hat{y_i} = \text{predicted}$$

### Sum of Squared Residuals (SSR)
The sum of the squared differences between each observed value and its associated predicted value. 
$$SSR = \sum_{i=1}^{n} ( y_i - \hat{y}_i )^2$$

### Oridnary Least Squares (OLS) 
In linear regression, we use OLS to get the best fitted line. OLS is a method that minimizes the error, or the sum of squared residuals to estimate parameters in a linear regression model. 

### Correlation coefficient (r, Pearson’s correlation coefficient)
The correlation coefficient quantifies the strength of the linear relationship between two variables. It always falls in the range of [-1, 1]. When r is negative, there is a negative correlation between the variables: as one increases, the other decreases. When r is positive, there is a positive correlation between the variables: as one increases, so too does the other. When r = 0, there is no linear correlation between the variables. 

![Correlation coefficient plots](images/correlation_coefficient_1.png)

Note that there are cases where one variable might be precisely determined by another—like y=x2 or y=sin(x)—but the value of the linear correlation between X and Y would nonetheless be low or zero because their relationship is non-linear.

However, r only tells you the strength of the linear correlation between the variables; it does not tell you anything about the magnitude of the slope of the relationship between the variables aside from its sign.

![Correlation coefficient plots](images/correlation_coefficient_2.png)

### Equations

## Multiple linear regression
## Advanced hypothesis testing

# Logistic regression
A technique that models a categorical dependent variable based on one or more independent variables. For example, if leads become subscriber or not, customer renew or not renew. The dependent variable can have two or more possible discrete values. 

In the case of logistic regression, the relationship between x and y doesn't forma straight line. 

![Logistic regression doesn't form a line. ](images/logistic_regression.png)

Therefore we need a different way to model the logistic regression. The key is to get the probability of y equals to 1 given x. 

$$\mu_{y|x} = \text{Prob}(Y=1|X)$$

An easy way to do it would be sum all binary y (0 or 1) and devide by the total count. 

However, you would want to link this equation to a regression equation with a formula. 

**The link function**

A nonlinear function that connects or links the dependent variable to the independent variables mathematically. 

$$g(p)=\beta_0 + \beta_1 * X$$

## Linear regression VS Logistic regression


|Linear regression                   |Logistic regression                     |
|------------------------------------|----------------------------------------|
|Continious data (e.g. 1, 2, 3...)   |Categorial data (e.g. subscribed or not)|
|Estimate the mean of y              |Estimate the probability of an outcome  |

: Linear regression VS Logistic regression