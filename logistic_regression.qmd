---
title: Logistic regression
jupyter: python3
format: 
    html: 
        code-fold: False
        toc: True
        toc-location: left
---

# About
This notebook aims to demonstrate the maths behind the Logistic Regression model with a synthetic dataset. 

# Setup
## Packages
```{python}
# Import libraries
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report



```

## Dataset (created with ChatGPT)
```{python}
np.random.seed(42)
time_on_website = np.random.randint(1, 30, 50) 

purchase_prob = 1 / (1 + np.exp(-0.6 * (time_on_website - 15)))
purchased = np.random.binomial(1, purchase_prob)

data = {
    "Time_on_Website": time_on_website,
    "Purchased": purchased
}
df = pd.DataFrame(data)

df
```

# Linear VS Logistic regression
To understand the logistic regression, let's first take a step back and see how a binary y variable fit into a linear regression model. 

```{python}
sns.regplot(x='Time_on_Website', y='Purchased', data=df); 
plt.grid()
```

The graph indicate that if a visitor spent 15 minutes on the website, the probability of them making a purchase it around 0.5. However, the problem of usinga linear regression in such scenario is that the result is not bound by the binary category of 1 and 0, so the output cannot be interpt as probability. For instance, the vistors that spent more than 25 minutes will have a y variable of more than 1, which is not valid. 

In order to understand the relationship between a continious x variable and a binary y variable, we would want a Sigmoid curve (the s line) like below: 

```{python}
sns.regplot(
    x='Time_on_Website', y='Purchased', 
    data=df, logistic=True); 
plt.grid()
```

# The Logit function
To understand the Sigmoid curve, first we need to look at the logit function. The logit function ensures that the output of the model is bounded between 0 and 1. It is defined as follow: 

$$logit(p) = ln(\frac{p}{1-p})$$

If we have a set of probability, and transform them with the logit function, you can see logit is only defined between the domain 0 and 1, and the range is from $-\infty$ to $\infty$.

```{python}
import pandas as pd
import numpy as np
import seaborn as sns

np.random.seed(42)
prob_list = np.random.random_sample(50)

prob_logit = pd.DataFrame({'probability': prob_list})

prob_logit['logit'] = np.log(prob_logit['probability']/(1-prob_logit['probability']))

prob_logit

fig=sns.lineplot(x='probability', y='logit', data=prob_logit)
fig.axhline(y=0, linestyle='--', color='gray')
fig.set_title('p-value VS logit(p)')
plt.show()
```

Now let's transform the logit function into an equation with based on the p-value. 

$$logit(p) = ln(\frac{p}{1-p}) = \beta_0 + \beta_1 * x_1$$
$$ln(\frac{p}{1-p}) = \beta_0 + \beta_1 * x_1$$
$$(\frac{p}{1-p}) = e^{\beta_0 + \beta_1 * x_1}$$
$$p = e^{\beta_0 + \beta_1 * x_1} - e^{\beta_0 + \beta_1 * x_1}*p$$
$$p + e^{\beta_0 + \beta_1 * x_1}*p = e^{\beta_0 + \beta_1 * x_1}$$
$$p(1 + e^{\beta_0 + \beta_1 * x_1}) = e^{\beta_0 + \beta_1 * x_1}$$
$$p = \frac{e^{\beta_0 + \beta_1 * x_1}}{1 + e^{\beta_0 + \beta_1 * x_1}}$$

# The Sigmoid function
It is a mathematical function that maps any real-valued input z to a value between 0 and 1. It is defined as: 

$$\sigma(z) = \frac{1}{1+e^{-z}}$$

Where: 
$$\sigma = Sigma$$

This is commonly used to represent the Sigmoid function. 

$$z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n$$

The Sigmoid function can also be written as: 

$$\sigma(z) = \frac{e^z}{1+e^z}$$

Because: 

$$e^{-z} = \frac{1}{e^z}$$

And

$$e^z * e^{-z} = e^0 = 1$$

In logistic regression, Instead of predicting directly from z, logistic regression maps z to a probability p using the sigmoid function, this ensures that p is always between 0 and 1, making it a valid probability: 

$$p = \sigma(z) = \frac{1}{1+e^{-z}}$$

Now if you put the Logit function and the Sigmoid function together, below is also a valid equation: 
$$ln(\frac{p}{1-p}) = \beta_0 + \beta_1 * x_1 = z$$

And since $z = \beta_0 + \beta_1 x_1$
$$p = \beta(z) = \frac{1}{1+e^{-z}} = \frac{e^z}{1+e^z} = \frac{e^{\beta_0 + \beta_1 * x_1}}{1 + e^{\beta_0 + \beta_1 * x_1}}$$

Now let's look at how the Sigmoid function transform the z value to valid probability: 

```{python}
np.random.seed(42)
z_list = np.random.randint(-20, 20, 40)

def sigmoid_function(x): 
    p = 1/(1+math.exp(-x))
    return p

sigmoid_sample = pd.DataFrame({'z': z_list})

sigmoid_sample['p'] = sigmoid_sample['z'].apply(sigmoid_function)

sigmoid_sample

sigmoid_sample = sigmoid_sample.sort_values(by='z')

sns.lineplot(x='z', y='p', data=sigmoid_sample);
```

The sigmoid function, as the name suggests, takes on an S-looking shape. The domain z stretches from $-\infty$ to $\infty$, while the range is bounded between 0 and 1. The output of the sigmoid function therefore can be interpreted as a probability. 

# Maximum likelihood estimation (MLE)
A technique for estimating the beta parameters that maximize the likelihood of the model prodcuing the observed data. 

- **Likelihood**: 

The probability of observing the actual data, given some set of beta parameters. 

# Binomial logistic regression
A technique that models the probability of an observation falling into one of two categories, based on one or more independent variables. 

## Odds and Logit
- **Odd**: 

$$\text{odd} = \frac{P}{1-P}$$

- **Logit** (log-odds): 

The logarithm of the odds of a given probability. So the logit of the probability p is equal to the logarithm of p divided by 1 minus p. 

$$\text{logic}(p) = \text{log}(\frac{p}{1-p})$$

- **Logit in terms of X variables**: 

$$\text{logic}(p) = \text{log}(\frac{p}{1-p}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n$$

## Assumptions
- **Linearity**: 

There should be a linear relationship between each X variable and the logit of the probability that Y equals 1. 

- **Independent observation**: 

While observation are independent: 

$$P(A and B) = P(A)*P(B)$$

- **No multicollinearity**: 

The multiple X variables should not be highly correlated to one another. 

- **No extreme outliner**

## Training a Binomial logistic regression model with Python

## Evaluate the trained model

# Extra

```{python}
# Independent (X) and dependent (y) variables
X = df[['Time_on_Website']]  # Features
y = df['Purchased']    # Target

# Step 2: Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 3: Train Logistic Regression Model
# Initialize and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Step 4: Make Predictions
# Predict on the test set
y_pred = model.predict(X_test)
y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probability of "1"

# Step 5: Evaluate the Model
# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
classification_report_text = classification_report(y_test, y_pred)

# Step 6: Visualize the Sigmoid Curve
# Generate sigmoid curve data
x_values = np.linspace(0, 10, 100).reshape(-1, 1)  # Generate values from 0 to 10
y_values = model.predict_proba(x_values)[:, 1]    # Predict probabilities for these values

# Plot sigmoid curve and data points
plt.figure(figsize=(8, 6))
plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(x_values, y_values, color='red', label='Sigmoid Curve')
plt.xlabel('Time on Website (minutes)')
plt.ylabel('Probability of Buying Lemonade')
plt.title('Logistic Regression Sigmoid Curve')
plt.legend()
plt.grid(True)
plt.tight_layout()

# Show the plot
plt.show()

# Display evaluation metrics and predictions
accuracy, y_pred, y_pred_prob, classification_report_text

```
```{python}
import sklearn.metrics as metrics

cm = metrics.confusion_matrix(y_test, y_pred, labels = model.classes_)

disp = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels = model.classes_)

disp.plot()

```